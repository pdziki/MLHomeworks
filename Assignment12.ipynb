{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/dzikipm1/Downloads/creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-0.694242</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>1.672773</td>\n",
       "      <td>0.973366</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>0.347068</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.082637</td>\n",
       "      <td>0.331128</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.382854</td>\n",
       "      <td>-0.176911</td>\n",
       "      <td>0.110507</td>\n",
       "      <td>0.246585</td>\n",
       "      <td>-0.392170</td>\n",
       "      <td>0.330892</td>\n",
       "      <td>-0.063781</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1.996583</td>\n",
       "      <td>0.608496</td>\n",
       "      <td>0.161176</td>\n",
       "      <td>0.109797</td>\n",
       "      <td>0.316523</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>-0.061820</td>\n",
       "      <td>-0.063700</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>-0.232494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.307377</td>\n",
       "      <td>-0.880077</td>\n",
       "      <td>0.162201</td>\n",
       "      <td>-0.561131</td>\n",
       "      <td>0.320694</td>\n",
       "      <td>0.261069</td>\n",
       "      <td>-0.022256</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.693500</td>\n",
       "      <td>-0.811578</td>\n",
       "      <td>1.169468</td>\n",
       "      <td>0.268231</td>\n",
       "      <td>-0.364572</td>\n",
       "      <td>1.351454</td>\n",
       "      <td>0.639776</td>\n",
       "      <td>0.207373</td>\n",
       "      <td>-1.378675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337632</td>\n",
       "      <td>1.063358</td>\n",
       "      <td>1.456320</td>\n",
       "      <td>-1.138092</td>\n",
       "      <td>-0.628537</td>\n",
       "      <td>-0.288447</td>\n",
       "      <td>-0.137137</td>\n",
       "      <td>-0.181021</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.493325</td>\n",
       "      <td>-0.112169</td>\n",
       "      <td>1.182516</td>\n",
       "      <td>-0.609727</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.936150</td>\n",
       "      <td>0.192071</td>\n",
       "      <td>0.316018</td>\n",
       "      <td>-1.262503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147443</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.304777</td>\n",
       "      <td>-1.941027</td>\n",
       "      <td>1.241904</td>\n",
       "      <td>-0.460217</td>\n",
       "      <td>0.155396</td>\n",
       "      <td>0.186189</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-0.591330</td>\n",
       "      <td>0.531541</td>\n",
       "      <td>1.021412</td>\n",
       "      <td>0.284655</td>\n",
       "      <td>-0.295015</td>\n",
       "      <td>0.071999</td>\n",
       "      <td>0.479302</td>\n",
       "      <td>-0.226510</td>\n",
       "      <td>0.744326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012839</td>\n",
       "      <td>1.100011</td>\n",
       "      <td>-0.220123</td>\n",
       "      <td>0.233250</td>\n",
       "      <td>-0.395202</td>\n",
       "      <td>1.041611</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.651816</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -1.996583 -0.694242 -0.044075  1.672773  0.973366 -0.245117  0.347068   \n",
       "1 -1.996583  0.608496  0.161176  0.109797  0.316523  0.043483 -0.061820   \n",
       "2 -1.996562 -0.693500 -0.811578  1.169468  0.268231 -0.364572  1.351454   \n",
       "3 -1.996562 -0.493325 -0.112169  1.182516 -0.609727 -0.007469  0.936150   \n",
       "4 -1.996541 -0.591330  0.531541  1.021412  0.284655 -0.295015  0.071999   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.193679  0.082637  0.331128  ... -0.024923  0.382854 -0.176911  0.110507   \n",
       "1 -0.063700  0.071253 -0.232494  ... -0.307377 -0.880077  0.162201 -0.561131   \n",
       "2  0.639776  0.207373 -1.378675  ...  0.337632  1.063358  1.456320 -1.138092   \n",
       "3  0.192071  0.316018 -1.262503  ... -0.147443  0.007267 -0.304777 -1.941027   \n",
       "4  0.479302 -0.226510  0.744326  ... -0.012839  1.100011 -0.220123  0.233250   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  class  \n",
       "0  0.246585 -0.392170  0.330892 -0.063781  0.244964      0  \n",
       "1  0.320694  0.261069 -0.022256  0.044608 -0.342475      0  \n",
       "2 -0.628537 -0.288447 -0.137137 -0.181021  1.160686      0  \n",
       "3  1.241904 -0.460217  0.155396  0.186189  0.140534      0  \n",
       "4 -0.395202  1.041611  0.543620  0.651816 -0.073403      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "classCol = df['Class']\n",
    "df = df.drop(columns=['Class'])\n",
    "scaler = StandardScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "df['class'] = classCol\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c districtdatalabs yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11b644190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "%matplotlib inline\n",
    "visualizer = ClassBalance()\n",
    "visualizer.fit(df['class']) \n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot shows that the data is heavily skewed towards being classified as 0, which makes class of 1 hard to detect/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:, df.columns != 'class'].values\n",
    "y = df.loc[:, df.columns == 'class'].values.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.506, C=      0.00001, TPR 0.214, FPR 0.000\n",
      "Test Accuracy= 0.977, C=      0.00010, TPR 0.560, FPR 0.000\n",
      "Test Accuracy= 0.980, C=      0.00100, TPR 0.762, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As C value increases for regularization, performance for the SVC gets better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.94%\n",
      "[[142145     11]\n",
      " [    76    172]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.980, TPR 0.758, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT2 (pruned) accuracy= 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Alpha beta pruning constant\n",
    "Alpha = 0.1\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.ccp_alpha = Alpha\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.5, stratify=y, random_state=0)\n",
    "clf2 = tree.fit(X_tr, y_tr)\n",
    "print(f'DT2 (pruned) accuracy= {accuracy_score(y_ts, clf2.predict(X_ts)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning increased accuracy of decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.91%\n",
      "[[142136     20]\n",
      " [   115    133]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP classifier was already very accurate without regularization - performance stayed at the same level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC with varying train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.510, C=      0.00001, TPR 0.204, FPR 0.000\n",
      "Test Accuracy= 0.970, C=      0.00010, TPR 0.556, FPR 0.000\n",
      "Test Accuracy= 0.981, C=      0.00100, TPR 0.782, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=None)\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.499, C=      0.00001, TPR 0.000, FPR 0.000\n",
      "Test Accuracy= 0.929, C=      0.00010, TPR 0.534, FPR 0.000\n",
      "Test Accuracy= 0.977, C=      0.00100, TPR 0.788, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7,random_state=None)\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.463, C=      0.00001, TPR 0.000, FPR 0.000\n",
      "Test Accuracy= 0.640, C=      0.00010, TPR 0.432, FPR 0.000\n",
      "Test Accuracy= 0.985, C=      0.00100, TPR 0.782, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8,random_state=None)\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.998, C=      0.00001, TPR 0.000, FPR 0.000\n",
      "Test Accuracy= 0.485, C=      0.00010, TPR 0.329, FPR 0.000\n",
      "Test Accuracy= 0.972, C=      0.00100, TPR 0.738, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9,random_state=None)\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.998, C=      0.00001, TPR 0.000, FPR 0.000\n",
      "Test Accuracy= 0.547, C=      0.00010, TPR 0.314, FPR 0.000\n",
      "Test Accuracy= 0.960, C=      0.00100, TPR 0.708, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95,random_state=None)\n",
    "\n",
    "#try svc with different c values\n",
    "SVM_C = [.00001, .0001, 0.001]\n",
    "FPR, TPR = [], []\n",
    "for c in SVM_C:\n",
    "    pipe_lr = make_pipeline(StandardScaler(),\n",
    "                            LogisticRegression(random_state=14, penalty='l1', solver='liblinear', class_weight='balanced',\n",
    "                                               C=c, multi_class='auto', max_iter=10000))\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    clf = svm.SVC(C = c, gamma = 'auto', kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    TPR += [tp/(tp+fn)]\n",
    "    FPR += [fp/(fp+tn)]\n",
    "    print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, C={c:13.5f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.960, TPR 0.791, FPR 0.001\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=None)\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.960, TPR 0.764, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7,random_state=None)\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.960, TPR 0.698, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8,random_state=None)\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.960, TPR 0.672, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9,random_state=None)\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy= 0.960, TPR 0.638, FPR 0.000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95,random_state=None)\n",
    "\n",
    "FPR, TPR = [], []\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "TPR += [tp/(tp+fn)]\n",
    "FPR += [fp/(fp+tn)]\n",
    "print(f'Test Accuracy= {pipe_lr.score(X_test, y_test):.3f}, TPR {TPR[-1]:.3f}, FPR {FPR[-1]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier with varying splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.90%\n",
      "[[170567     30]\n",
      " [   134    154]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6,random_state=None)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.88%\n",
      "[[198960     28]\n",
      " [   215    162]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7,random_state=None)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.91%\n",
      "[[227420     37]\n",
      " [   160    229]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8,random_state=None)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.90%\n",
      "[[255851     36]\n",
      " [   229    211]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9,random_state=None)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 99.92%\n",
      "[[270055     44]\n",
      " [   169    299]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95,random_state=None)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),max_iter=300, alpha = 1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## results in terms of training size, regularization, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I found that pruning/regularization yielded slightly better/more accurate results.  For this data set, even without manipulating the data in this way, the models used were already very accurate, and thus I didn't see much of a difference.  Likewise, with different sized training splits, performance was pretty steady.  That being said, regularization/pruning makes me more confident these models can be applied to new data - while without this data manipulation, we have accurate models, they are likely heavily biased towards this training data, and may not be able to be generalized to other data.  Regularization allows for better generalization to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
